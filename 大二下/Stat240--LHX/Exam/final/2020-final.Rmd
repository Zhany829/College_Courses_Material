---
output: html_document
geometry: margin=0.75in
fontsize: 12pt
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message=FALSE,
                      warning=FALSE,
                      cache = FALSE)
library(tidyverse)
library(lubridate)
library(stringr)
source("../../scripts/viridis.R")
```


# Fall 2020 STAT 240 Final Exam

#### Due Saturday, December 12, 2020 at 11:59 PM CT (local time in Madison, WI)

####  NAME:  Haoxuan Lu

### Preliminaries

- You will have 24 hours to complete the exam, and your solutions should be uploaded to Canvas by 11:59 PM CT (the time zone in Madison, WI).   

- You are not allowed to communicate with anyone using any means (email, phone, text, social media, online discussion platforms, etc.) except the instructors of this course.  You are allowed to use materials from the course and the internet.    

- If you have a question during the exam, post your question as a *private* post on Piazza.  To do this, select the "Individual Student(s) / Instructor(s)" option next to "Post to:" when creating your post. 

- It is recommended that you begin the exam as soon as possible and read over it to see if you have any questions.  You can expect for questions to be addressed during normal working hours in Madison, WI (9 AM CT - 5 PM CT).  Questions posted outside that window *may* still be addressed.

### Submission

Once you have completed the exam, knit the R Markdown document to create an HTML file.  To submit this Exam, go to our Canvas site and select "Assignments" on the left panel, and upload both the edited Rmd and HTML files to the place designated for the final exam.  *Be sure to review the HTML to verify all your answers appear as you expect.*

### Data

The following data files are needed to complete this exam.  More information about the data sets are provided later in the exam. 

Danish Family Data:  
*danish-children.csv*

Dollar General Data:  
*dollar_general_counts.csv*  
*georgia_population_by_county.csv*  
*georgia_fei_2020_actual.csv*  

Bike Trip Data:  
*trip.csv*




# Problems

The exam has a total of 15 problems, some with several parts, totaling 57 points.



## Short Answer Questions


### Problem 1 (2 points)

In a hypothesis test about an unknown parameter, the test statistic...   

**Select the best response**

(a) measures the compatibility between the null and alternative hypotheses
(b) is the value of the unknown parameter under the null hypothesis
(c) measures the compatibility between the null hypothesis and data
(d) is the value of the unknown parameter under the alternative hypothesis
(e) None of the above

- C



### Problem 2 (3 points)

A particle randomly moves along a line. That is, the particle starts at the origin (position 0) and moves either right or left in independent steps of length 0, 1, or 2 (where length 0 means the particle stays in its current location). 

The distribution of the particle’s movement at step i is listed in the table below called `x_distribution`. $X_i$ is the random variable with the distribution below, and $x_i$ are the possible values $X_i$ can take.  The probability of each $x_i$ is listed below under $P(X_i = x_i)$.
```{r}
x_distribution <- tibble(x_i = c(-2, -1, 0, 1),
                         `P(X_i=x_i)` = c(.5, .2, .1, .2))
x_distribution
```

The position of the particle after $k$ steps is the sum of these random movements 
$$
Y = X_1+X_2+...+X_k.
$$
The probability distribution of $Y$ (with computed values rounded to the nearest 0.01) is

(a) Exactly Normal with mean = $0$ and variance = $1.67/k$
(b) Exactly Normal with mean = $−0.5$ and variance = $1.67/k$
(c) Exactly Normal with mean = $−1$ and variance = $1.4/k$
(d) Approximately Normal with mean = $0$ and variance = $1.3/k$
(e) Approximately Normal with mean = $−0.5$ and variance = $1.4/k$
(f) Approximately Normal with mean = $−1$ and variance = $1.67/k$
(g) None of the above.

- G



### Problem 3 (3 points)

The weight, X, of cherry tomatoes selected at random from a very large bin at the local supermarket follows a Normal distribution with mean 3 oz. and standard deviation 2 oz. Suppose we pick 8 cherry tomatoes from the bin at random (independently) and put them in our bag. What is the probability that exactly 5 of the 8 cherry tomatoes weigh less than 4 oz (rounded to the nearest 0.01)?

```{r}
p = pnorm(4, 3, 2)
round(dbinom(5, 8, p), 2)
```



### Problem 4 (3 points)

Suppose a confidence interval for the mean weight increase of pregnant women in their first 5 months of pregnancy is found to be 16 +/- 0.134 lbs based on a random sample of size n = 400. Assuming the population standard deviation is known to be 2 lbs, what confidence level was used to calculate this confidence interval (rounded to the nearest %)?

```{r}
se <- 2 / sqrt(400)
round((pnorm(16 + 0.134, 16, se) - pnorm(16 - 0.134, 16, se)) * 100, 0)
```




## Danish Family Questions

Data for the next set of questions is in the file *danish-children.csv*
which was collected from 1960 to 1994 from all Danish families 
with only single births.
Each row is a summary of the number of children in a family in the data set
with a given birth order, sex, and the sequence of the sexes of any previous children in the family.
There are four columns in this data set.

- `order` - the birth order;
- `sex` - F or M depending on sex assigned at birth;
- `previous` - the sequence of sexes of previous children in the family;
- `n` - the count of the number of such children

```{r}
danish <- read_csv("../../data/danish-children.csv")
danish
```

### Problem 5 (3 points)

Define $p_1$ to be the probability that a third child is a girl among families who have a third child after first having two girls.
Let $p_2$ be the probability that a third child is a girl among families who have a third child after first having two boys.
Treat the observed children in this data set as a sample from a population of potential children that might have been born in these families.
Determine the data below and estimate these two probabilities:

- $n_1$ is the number of families that have a third child after having two girls.
- $x_1$ is the number of girls from such families.
- $n_2$ is the number of families that have a third child after having two boys.
- $x_2$ is the number of girls from such families.

```{r}
first_two_girls <- danish %>%
  filter(previous == "FF") %>%
  mutate(p = n/sum(n))
first_two_boys <- danish %>%
  filter(previous == "MM") %>%
  mutate(p = n/sum(n))

data <- tibble(
  p_1 = first_two_girls$p[1],
  p_2 = first_two_boys$p[1],
  n_1 = sum(first_two_girls$n),
  x_1 = first_two_girls$n[1],
  n_2 = sum(first_two_boys$n),
  x_2 = first_two_boys$n[1]
)
data
```

Data shown as tibble above.


### Problem 6 (5 points)

**(a)** Find a 95% confidence interval for $p_1 - p_2$.    
**(b)** Interpret the meaning of this interval in context.

```{r}
se = sqrt(data$p_1 * (1 - data$p_1) / data$n_1 + data$p_2 * (1 - data$p_2) / data$n_2)
c(data$p_1 - data$p_2 - qnorm(0.975) * se, data$p_1 - data$p_2 + qnorm(0.975) * se)
```

- This interval means we are 95% confident that the true difference p1−p2 is between 0.0006 and 0.0142



### Problem 7 (8 points)

Test the null hypothesis that $p_1 = p_2$ versus the two-sided alternative.

**(a)** State hypotheses    
$$
H_0: P_1 - P_2 = 0
$$
$$
H_1: P_1 - P_2 \neq 0
$$

**(b)** Calculate a test statistic    
**(c)** Report a p-value    
**(d)** Interpret the result of the test in the context of the problem.

```{r}
z_score <- (data$p_1 - data$p_2) / se
z_score
p_value <- pnorm(z_score, lower.tail=FALSE) * 2
p_value
```

- The p-value is 0.032 < 0.05, so we have the evidence to reject the null hypothesis. Thus, the p1 and p2 are different.





## Dollar General Questions

Dollar General is a chain of budget stores that has grown rapidly across the country at a time when many other retailers are shrinking.
Part of their formula of success is to sell inexpensive food which is often processed and unhealthy, often undercutting local stores which sell more expensive and healthy food options, sometimes driving these competitors out of business.

Researchers have developed the Food Environment Index (FEI) to measure the food environment in neighborhoods. The FEI combines a measure of proximity to healthy food among low income residents
with a measure of food insecurity and takes values on a scale from 0 (worst) to 10 (best).
Values in a county are averaged over values in smaller units within the county.
In 2020, the median value for counties in the United States was 7.6
and most counties fell between about 6.9 and 8.2.
For comparison,
in Wisconsin, the mean across counties is 8.5 and most (plus or minus one standard deviation) are between 8.0 and 9.0 and the value in Dane county,
in which Madison is situated, is 8.2.

One complaint about Dollar General stores is that when a new store opens in a small town, local grocers compete poorly and are put out of business,
causing people to lose access to local healthy food options.
The following questions examine an association between the FEI measured at the county level and the number of Dollar General stores per capita in counties in the state of Georgia.

Here are variable descriptions:

*dollar_general_counts.csv*

- `County` - the name of a county in Georgia
- `Dollar General` - the number of Dollar General stores in the county

*georgia_population_by_county.csv*  

- `County` - the name of a county in Georgia
- `population` - the population of the county

*georgia_fei_2020_actual.csv*  

- `FIPS` - a unique county identifier code used by federal agencies
- `State` - the state name (always Georgia for this data)
- `County` - the county name
- `Food Environment Index` - the average FEI value for the county

```{r}
dollar <- read_csv("../../data/dollar_general_counts.csv") %>%
  mutate( County = case_when(
      County=="Dekalb" ~ "DeKalb",
      County=="Mcduffie" ~ "McDuffie",
      County=="Mcintosh" ~ "McIntosh",
      TRUE ~ County
      ))
georgia_pop <- read_csv("../../data/georgia_population_by_county.csv")
georgia_fei<- read_csv("../../data/georgia_fei_2020_actual.csv")
```


### Problem 8 (4 points)

Combine and transform data from the files *dollar_general_counts.csv*, *georgia_population_by_county.csv*, and *georgia_fei_2020_actual.csv* into a single data frame named `dg` with one row per county and with these columns:

- *county*: the name of the county
- *fei*: the Food Environment Index value
- *population*: the population of the county
- *stores*: the number of Dollar General stores
- *dg_per*: the number of Dollar General stores per 100,000 people.

Note that the state of Georgia has 159 counties.
Any county that does not appear in the file *dollar_general_counts.csv* has no Dollar General Stores.

After creating the data frame, create and print a summary of the median values for each of the four quantitative variables as a tibble with a single row, discarding any missing observations in variables which have them, but using all non-missing data for each variable.

```{r}
dg <- dollar %>%
  right_join(georgia_fei) %>%
  right_join(georgia_pop) %>%
  mutate(county = County, 
         fei = `Food Environment Index`,
         stores = `Dollar General`,
         dg_per = stores / population * 100000) %>%
  select(county, fei, population, stores, dg_per)
dg

tibble(
  fei_med = median(dg$fei, na.rm = TRUE),
  pop_med = median(dg$population, na.rm = TRUE),
  sto_med = median(dg$stores, na.rm = TRUE),
  dgp_med = median(dg$dg_per, na.rm = TRUE)
)
```



### Problem 9 (4 points)

**(a)** Create a scatter plot with the number of Dollar General stores per 100,000 people on the x axis and FEI on the y axis.
Indicate the county populations with the size of the points and set `alpha = 0.5` to make points partially transparent to lessen the visual impact of overplotting.
Add a straight regression line to the plot.

**(b)** What is the slope of the regression line, rounded to three significant digits?

```{r}
ggplot(dg, aes(x = dg_per, y = fei)) +
  geom_point(aes(size = population), alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_bw()

coef_1 <- coef(summary(lm(fei ~ dg_per, dg)))
signif(coef_1[2], 3)
```

- Slope is -0.0390



### Problem 10 (4 points)

The two counties with FEI values below three are outliers and potentially influential points for the fitted regression model.

**(a)** After removing these two points, replot the data and fit a regression line as before.

**(b)** Report the slope of this line, rounded to three significant digits.

```{r}
dg %>%
  filter(fei >= 3) %>%
  ggplot(aes(x = dg_per, y = fei)) +
  geom_point(aes(size = population), alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_bw()

coef_2 <- coef(summary(lm(fei ~ dg_per, filter(dg, fei >= 3))))
signif(coef_2[2], 3)
```

- Slope is -0.0199


### Problem 11 (3 points)

One county has missing data for the FEI.

**(a)** Identify this county and report the values of its non-missing data.

**(b)**
Use each of the two regression models (using all data and with the two low FEI values removed) to predict the missing FEI value for this county, rounded to one decimal place.

```{r}
missing <- dg %>%
  filter(is.na(fei))
missing

fei_1 <- coef_1[2] * missing$dg_per + coef_1[1]
round(fei_1, 1)
fei_2 <- coef_2[2] * missing$dg_per + coef_2[1]
round(fei_2, 1)
```

- The predict fei is 7.1 for the regression model without removing outliers and 7.2 for the regression model removing the outliers.




## Bike Trip Questions

The Bike Trip data are contained in the *trip.csv* file.  The data were made available by the Bay Area Bike Share service and accessed through UC-Berkeley's Data8 site, and include bike rentals from this program between September 1, 2014 and August 31, 2015.  More information, including more extensive data, is available [here](https://www.sfmta.com/getting-around/bike/bike-share).

The data include the following variables:

- `Trip ID` - an identifer for each bike trip  
- `Duration` - the length of the use of the bike in seconds  
- `Start Date` - the date and time the bike trip started  
- `Start Station` - the name of the station at which the bike trip started  
- `Start Terminal` - the terminal number at the start of the bike trip  
- `End Date` - the date and time the bike trip ended  
- `End Station` - the name of the station at which the bike trip ended  
- `End Terminal` - the terminal number at the end of the bike trip  
- `Bike #` - the serial number identifying the bike  
- `Subscriber Type` - indicated as "Subscriber" (i.e., a member of the bike sharing program) or "Customer" (a casual user of the bike sharing program)  
- `Zip Code` - zip code of the user 



### Problem 12 (4 points)

Define a data frame called `trip` by doing the following.  Read in the *trip.csv* data set and rename the variables as listed below:

id = `Trip ID`,  
duration = `Duration`,  
start_date = `Start Date`,  
end_date = `End Date`,  
start_station = `Start Station`,  
end_station = `End Station`,  
start_terminal = `Start Terminal`,  
end_terminal = `End Terminal`,  
bike = `Bike #`,  
subscriber = `Subscriber Type`,  
zip = `Zip Code`  

Additionally, add a variable `year` indicating the year of the start date of the bike trip, and add a variable `month` indicating the month of the start date of the bike trip.  

**(a)** Use `print.data.frame(head(trip,6))` to print out the first 6 rows and all the columns of your `trip` data frame.

**(b)** Create a summary table that displays the minimum and maximum trip duration for each month.  There should be 3 columns and 12 rows in this table.  Print out this table.


```{r}
trip <-  read_csv("../../data/trip.csv") %>%
  rename(id = `Trip ID`,  
         duration = `Duration`,  
         start_date = `Start Date`,  
         end_date = `End Date`,  
         start_station = `Start Station`,  
         end_station = `End Station`,  
         start_terminal = `Start Terminal`,  
         end_terminal = `End Terminal`,  
         bike = `Bike #`,  
         subscriber = `Subscriber Type`,  
         zip = `Zip Code`) %>%
  mutate(year = year(mdy_hm(start_date)),
         month = month(mdy_hm(start_date)))

print.data.frame(head(trip,6))

trip_summary <- trip %>%
  group_by(month) %>%
  summarise(min_trip_dur = min(duration), max_trip_dur = max(duration))
print.data.frame(trip_summary)
```




### Problem 13 (3 points) 

Using your `trip` data frame, how many of the bike trips started at a Caltrain station?  You can assume that all the Caltrain stations have "Caltrain" in the name of the station.

```{r}
trip %>%
  filter(str_detect(start_station, "Caltrain")) %>%
  summarise(count = n())
```

- There are 60079 bike trips started at a Caltrain station.


### Problem 14 (4 points)

Using your `trip` data frame, create a new data frame called `trip2` that has the start stations listed in the first column, the end stations listed across the rest of the columns, and the content of the table indicating the number of bike trips between each start and end station.  For example, the table cell at row i and column j of the table should indicate how many bike trips took place between start station i and end station j.  If there were no trips between two stations, the value of the table should be `0`.

Sort the rows in alphabetical order by start station.  Note that start station names that begin with numbers can appear before start station names that begin with the letter "A."

**(a)** Print out the first 10 rows of the table and at least 3 columns.

**(b)** How many start and end stations did not have any trips taken?  That is, how many 0's are in the `trip2` table?

```{r}
trip2 <- trip %>% 
  select(start_station, end_station) %>%
  group_by(start_station, end_station) %>%
  summarise(count = n()) %>%
  pivot_wider(names_from = end_station, values_from = count, values_fill = 0) %>%
  arrange(start_station)

print.data.frame(trip2[1:10, 1:4])

sum(trip2 == 0)
```

- There are 3208 start and end stations did not have any trips taken



### Problem 15 (4 points)

Make a plot that displays the average duration for bike trips in each of the four seasons, where the seasons are specified as 

- Spring = March, April, May  
- Summer = June, July, August  
- Fall = September, October, November  
- Winter = December, January, February  

Use a thin blue vertical line segment to visualize a 95% confidence interval for the mean for each season, a slightly thicker black segment to visualize the interval one standard error above and below the point estimate (i.e., the sample mean) for each season, and a point at the point estimate for each season. Label axes appropriately and add an informative title to the plot.

```{r}
trip %>%
  mutate(season = case_when(month%in%c("3","4","5") ~ "Spring",
                            month%in%c("6","7","8") ~ "Summer",
                            month%in%c("9","10","11") ~ "Fall",
                            month%in%c("12","1","2") ~ "Winter")) %>%
  group_by(season) %>%
  summarize(mean = mean(duration),
            n = n(),
            se = sd(duration) / sqrt(n)) %>%
  ggplot() +
  geom_pointrange(aes(x = season, y = mean, ymin = mean - qnorm(0.975) * se, ymax = mean + qnorm(0.975)*se), color = "blue", size = 0.9) +
  geom_segment(aes(x = season, xend = season, y = mean - se, yend = mean + se), color = "black", size = 1.3) +
  labs(x = "Season", y = "Duration", title = "Average Duration for Bike Trips for each Season") +
  theme_bw()
```








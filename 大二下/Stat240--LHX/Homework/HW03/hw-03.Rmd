---
title: "Assignment 3"
author: "Haoxuan Lu"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, fig.height = 3)
library(tidyverse)
library(lubridate)
## Note: this code assumes viridis.R is two steps up the file directory tree
## Comment it out and use the line below if the file is in the same directory
## Or comment out both lines if you do not have the file
source("../../scripts/viridis.R")
```

#### Due Friday, September 18 at 11:59 PM CT

```{r read-data, echo = FALSE}
## Read the US deaths data
## Read the US deaths data
us_deaths = read_csv("../../data/us-deaths.csv") %>% 
  select(-baseline) %>% 
  mutate(week_end_date = mdy(week_end_date))

## Read the S&P 500 data
sp500 = read_csv("../../data/sp500.csv") %>% 
  select(-`Adj Close`,-Volume)
```

### Problems

Choose to do either the set of three problems 1A, 2A, and 3A with the US Death data or the three problems 1B, 2B, 3B with the S&P 500 data.

### 1A

Based on death counts and population and demographic data, a department within the US Center for Disease Control (CDC) makes a prediction interval for what the total number of US deaths is expected to be.
This interval accounts for changes in the population, the age structure, and several other demographic variables.
The prediction interval accounts for variation at typical levels judged over many years.
When the actual death count exceeds the maximum of the prediction interval for several consecutive weeks,
this is evidence that there is some cause of excess deaths.

The following code is a simple line plot of the actual deaths and the maximum number of predicted deaths under normal conditions since January, 2017.
Note that the command `scale_x_date()` is useful to have more control over the labeling of dates on the x-axis.
Here, the pattern "%b-%Y" specifies pattern with an abbreviation for the month, a dash, and the four-digit year.
See the help page by typing `?scale_x_date` for more details.

```{r problem-1a}
ggplot(us_deaths, aes(x = week_end_date)) +
  geom_line(aes(y = actual_deaths), color = viridis3[1]) +
  geom_line(aes(y = maximum_expected_deaths), color = viridis3[3]) +
  scale_x_date(date_labels = "%b-%Y")
```

>What do you thing explains the cyclical pattern in the line plot of the upper end of the prediction interval? How does the death rate tend to change with season?

The data show that the weekly total number of deaths in the US has an up and down cyclical pattern just like a sine function, which always increase from summer to winter and reach the peak near January, and decrease from winter to summer and down to the bottom near July and August for the last three years. This pattern may be caused by the low temperature in the winter. The human's immune system reduces because of severe cold weather, and the risk of deadly cardiovascular diseases or respiratory problems such as high blood pressure, heart disease, stroke, or pneumonia increase.

### 2A

Make changes to the code so that:

- The value zero appears on the y-axis so we can better compare relative rates.
- The values on the y-axis are divided by 10,000
- There are clear and informative axis labels and a title.

```{r problem-2a}
ggplot(us_deaths, aes(x = week_end_date)) +
  geom_line(aes(y = actual_deaths / 10000), color = viridis3[1]) +
  geom_line(aes(y = maximum_expected_deaths / 10000), color = viridis3[3]) +
  scale_x_date(date_labels = "%b-%Y") +
  scale_y_continuous(limits = c(0, 8)) +
  ggtitle("The Predicted and Actual Deaths in the US from 2017 to 2020") +
  labs(x = "Time Period (by week)", y = "Weekly Death (10000 of people)")
```

> When are the two time periods when the number of actual death greatly exceeds the maximum expected number of deaths over several consecutive weeks?

1. The January to February of 2018
2. The March of 2020 to now

### 3A

```{r problem-3a}
us_deaths %>% 
  filter(week_end_date > ymd("2020-01-01") & week_end_date < ymd("2020-08-01")) %>% 
ggplot(aes(x = week_end_date)) +
  geom_col(aes(y = actual_deaths), color = viridis3[1], fill = viridis3[1]) +
  geom_line(aes(y = maximum_expected_deaths), color = viridis3[3]) +
  scale_x_date(date_labels = "%b-%Y", date_break = "1 month") +
  xlab("") +
  ylab("Actual Deaths") +
  ggtitle("US Deaths in 2020 by Week") +
  theme_bw() +
  theme(axis.text.x = element_text(angle=60, hjust=1)) 
```

>When during the year did the number of actual deaths begin to greatly exceed the maximum expected number?
>When did the number of excess deaths peak?
>Is the current trend of excess deaths increasing or decreasing?

- The second half of the March, around March 22rd, 2020, the number of actual deaths begin to greatly exceed the maximum expected number.
- The beginning of the April, around April 10th, 2020, the number of excess deaths peak.
- The current trend of excess death is relatively slow and steady comparing to the peak periods but still increasing.

### 1B

The following code is a simple line plot of the S&P 500 Index closing value and the daily high.

```{r problem-1b}
ggplot(sp500, aes(x = Date)) +
  geom_line(aes(y = Close), color = viridis3[1]) +
  geom_line(aes(y = High), color = viridis3[3]) +
  scale_x_date(date_labels = "%b-%Y")
```

> Provide a description of the pattern you see.

RESPONSE

### 2B

Make changes to the code so that:

- The value 0 appears on the y-axis.
- There are clear and informative axis labels and a title.
- Change the colors of the lines to something different.

```{r problem-2b}
ggplot(sp500, aes(x = Date)) +
  geom_line(aes(y = Close), color = viridis3[1]) +
  geom_line(aes(y = High), color = viridis3[3]) +
  scale_x_date(date_labels = "%b-%Y")
```

About what percentage of the value of S&P dropped from its peak in mid March to the lowest point in late March?

### 3B

```{r problem-3b}
ggplot(sp500, aes(x = Date)) +
  geom_col(aes(y = Close), color = viridis3[1]) +
  geom_line(aes(y = High), color = viridis3[3]) +
  scale_x_date(date_labels = "%b-%Y", date_break = "1 month") +
  xlab("") +
  ylab("S&P 500 Index") +
  ggtitle("2020 S&P Index") +
  theme_bw() +
  theme(axis.text.x = element_text(angle=60, hjust=1)) 
```

Make the plot as above and again with the `scale_x_date()` line commented out.

> What do you think this line of code does?

RESPONSE

> Repeat for the last line of code with `theme(...)`

RESPONSE

### 4

> Explain the difference between the `geom_col()` and `geom_bar()` functions.

- If the data represent the height of the bars is not pre-counted in the data frame, we use the "geom_bar()" function. 
- If the data represent the height of the bars is pre-counted and stored in the data frame, we use the "geom_cal()" function with the     y-position aesthetic mapped to the variable that has the counts.

### 5

```{r read-planet-data}
## Read in the csv file
## There is one row per exoplanet after applying `filter(default_flag != 0)`
## Select some variables that we will work with and rename them
## Remove very massive planet (only to improve plot visuals)
## Drop missing values; the remaining exoplanets will have estimates of both mass and radius
planets <- read_csv("../../data/exoplanets-3sept2020.csv") %>%
  filter(default_flag != 0) %>%
  select(pl_name, discoverymethod, disc_year, sy_pnum, pl_rade, pl_bmasse) %>%
  rename(name=pl_name, method=discoverymethod,year=disc_year, number=sy_pnum, radius=pl_rade, mass=pl_bmasse) %>%
  filter(mass <10000) %>%
  drop_na() 
```

The following block of code make two different scatter plots of mass on a log scale versus year.

```{r}
ggplot(planets, aes(x = year, y = mass, color = method)) +
  geom_point() +
  scale_y_log10() +
  xlab("Discovery Year") +
  ylab("Mass (Earth Mass)") +
  ggtitle("Exoplanets Mass vs. Discovery Year",
          subtitle="Using scale_y_log10()")

ggplot(planets, aes(x = year, y = log10(mass), color = method)) +
  geom_point() +
  xlab("Discovery Year") +
  ylab("log10(Mass/Earth Mass)") +
  ggtitle("Exoplanets Mass vs. Discovery Year",
          subtitle="using y = log10(mass)")
```

>Describe the differences between the two plots.
>Which do you think is more effective and why?

- The differences between the two plots are caused by how the masses of the exoplanets represented in the charts. The first plot directly use the exact value of each exoplanets, while the second plot takes the logarithm of 10 for the mass of exoplanets. I think the second plot is more effective because the first plot has an uneven scale which means we can hardly directly get the data from the plot with our eyes. The maximum and the minimum are thousands of times different, so I think it will be better to take the logarithm of 10 on data in order to reach an even scale as the second plot.

### 6

Using the exoplanet data,
make a plot with discovery year on the x axis,
mass on the log10 scale on the y-axis,
and with a different facet for each method.
Set the argument `position` within the `geom_point()` command using the command `position_jitter()` so that points are jittered horizontally, but not vertically.
Choose an amount of jitter so that there is a reduction in overplotting,
but all points for a single year appear as a band without overlap with other years.

```{r, echo=TRUE}
ggplot(planets, aes(x = year, y = log10(mass), color = method)) +
  geom_point(position = position_jitter(width = 0.05, height = 0), size = 0.3) +
  facet_wrap(~method) +
  xlab("Discovery Year") +
  ylab("log10(Mass/Earth Mass)") +
  ggtitle("Exoplanets Mass vs. Discovery Year",
          subtitle="using y = log10(mass)")
```

### 7

With the exoplanet data,
make a bar graph of the method variable that displays the count of the number of observations for each method.

```{r, echo=TRUE}
ggplot(data = planets) +
  geom_bar(mapping = aes(x = method)) +
  xlab("Method") +
  ylab("The count of observations") +
  ggtitle("The Count of the Number of Exoplanets Observations for each Method") +
  theme(plot.title = element_text(size = 10), axis.text = element_text(size = 6))
```

### 8

Repeat the previous problem, but replace the counts on the y-axis with proportions.
(*Hint:* You need to set the `y` aesthetic to `stat(prop)` and the group aesthetic to `1`. See the second example in *R for Data Science* section 3.7.)

```{r, echo = TRUE}
ggplot(data = planets) +
  geom_bar(mapping = aes(x = method, y = stat(prop), group = 1)) +
  xlab("Method") +
  ylab("The proportion of observations") +
  ggtitle("The Proportion of Different Exoplanets Observations Method") +
  theme(plot.title = element_text(size = 11.5), axis.text.x = element_text(size = 6))
```

### 9

The following block of code reads in the raw Lake Mendota data and makes several transformations.
Examine indicated sections of the code and answer the corresponding questions.

```{r read-mendota}
mendota_interval = read_csv("../../Data/lake-mendota-raw.csv") %>% 
## question (a) begin
  select(-days) %>% 
## question (a) end
## question (b) begin    
  drop_na() %>% 
## question (b) end
## question (c) begin    
  separate(winter,into = c("year1","year2"), remove = FALSE) %>%
  mutate(year1 = as.numeric(year1)) %>%
  mutate(year2 = year1+1) %>% 
## question (c) end    
  mutate(closed = case_when(
    str_detect(closed,"Oct|Nov|Dec") ~ str_c(closed,' ',year1),
    str_detect(closed,"Jan|Feb|Mar|Apr|May") ~ str_c(closed,' ',year2),
    TRUE ~ NA_character_
  )) %>%
  mutate(closed = dmy(closed)) %>%
  mutate(open = case_when(
    str_detect(open,"Oct|Nov|Dec") ~ str_c(open,' ',year1),
    str_detect(open,"Jan|Feb|Mar|Apr|May") ~ str_c(open,' ',year2),
    TRUE ~ NA_character_
  )) %>%
  mutate(open = dmy(open)) %>% 
  mutate(days = open - closed)

mendota = mendota_interval %>% 
## question (d) begin
  group_by(winter) %>% 
  summarize(intervals = n(),
            days = sum(days),
            first_freeze = min(closed),
            last_thaw = max(open)) %>%
## questions (d) end  
  mutate(year1 = as.numeric(str_sub(winter,1,4))) %>%
  mutate(decade = floor(year1 / 10) * 10) %>% 
  select(winter,year1,everything())
```

#### (a)

> What does the line `select(-days)` do to the data set?

- This line of code used to select all columns from the data set except "days" column.

#### (b)

>What does the command `drop_na()` do? How many rows (observations) are in the data set when it is first read in and how many rows remain after this code is executed?

- The command "drop_na()" uses to drop rows containing missing values. There are 175 rows are in the data set when it is first read in and 172 rows remain after this code is executed.

#### (c)

>Describe the effect these three lines of code have on the data set

- These three lines of code separate the beginning year and the ending year for each winter in the "winter" column to columns "year1" and "year2". Then, change the data in column "year1" from chr to dbl. Finally, utilize the data from column "year1" plus 1 to gain next year and store in the column "year2".

#### (d)

>Explain what the effect of these two commands are. In your response,
>describe what the effect of the `group_by(winter)` command is,
what the function `n()` does,
and what the functions `sum()`, `min()`, and `max()` do.

- These two commands use the "group_by(winter)" command to group the mendota_interval dataset by column "winter" and then apply the "summarize()" functions. The function "n()" gets the count of the number of rows in each "winter" group. The function "sum()" calculates the total amount of freeze days in this winter in each group and stores in the "day" column. The function "min()" finds the minimum (earliest) close date in the group and stores in the "first_freeze" column; and function "max()" finds the maximum (latest) open date in the group as the "last_thaw" column.
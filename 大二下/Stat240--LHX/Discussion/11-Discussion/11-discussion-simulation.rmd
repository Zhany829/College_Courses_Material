---
title: "STAT 240 Discussion 11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
library(tidyverse)
library(lubridate)
source("../../scripts/viridis.R")
source("../../scripts/ggprob.R")
```

## Group 319C

## Members Present

- **LILY FRANKS**
- MICHAEL HANDLER
- HAOXUAN LU
- SHRUTHI SRINIVASAN

## Members Absent

- ADD NAMES OF ANY ABSENT MEMBERS

## Background

We have used R many times in the course for simulations. Examples include:

- simulating the sampling distribution of a test statistic under the null hypothesis to calculate a p-value
- resampling a sample to estimate a standard error
- calculating the actual capture probability of a purported 95% confidence interval procedure

There are many problems in statistics where a mathematical derivation can be used to find a formula for a mean or a standard error or a probability. But there are many problems where the math is either too hard or does not exist, but a simulation can provide an accurate numerical estimate of the quantity of interest.

In this discussion assignment, you will write code to do simulations for several new examples.
A useful function is `sample()`.
A standard use is

```{r eval=FALSE}
x = seq(0, 1, 0.01) #I input this line 
sample(x, size=50, replace=TRUE)
```

which will take a sample with replacement of size 50 from a vector of values `x`.

## Example

> The code block below creates a data frame with a variable `wait` which represents the maximum waiting time among all US passport holders that arrive on an international flight at Chicago's O'Hare airport within an hour between the times of 8am and 8pm in 2018.
The data is the complete record and we will consider this to be our population.

> As an example, consider the sampling distribution of the median of a sample of size 50 from this population. We can calculate the median of the population, but what is the standard error of the sample median for a random sample of size 50?

```{r airport, fig.height=2}
## Read and format the data
chicago = read_csv2("../../data/chicago-2018.csv") %>%
  filter(hour >= "0800 - 0900" & hour <= "1900 - 2000") %>%
  select(us_max_wait) %>%
  rename(wait = us_max_wait)

## Plot the density
ggplot(chicago, aes(x=wait)) +
  geom_density(fill=viridis3[1]) +
  theme_bw()

## Calculate the population median
chicago %>%
  summarize(median = median(wait))

## Use simulation to estimate the sampling distribution of the sample median.

## Make a vector with the data we have from which we want to sample
chi_wait = chicago %>%
  pull(wait)

## Create the number of times we want to do the simulation
##   and the sample size as objects in R
B = 10000
n = 50

## Create a container to hold the sample median wait times
median_50 = numeric(B)

## Use a for loop to do the simulation
for ( i in 1:B )
{
  median_50[i] = median( sample(chi_wait, n, replace=TRUE) )
}

## The standard deviation of this sample is a numerical estimate of the standard error of the sampling distribution
se_50 = sd(median_50)
se_50

## Are about 95% of the sample medians within 1.96 SEs of the mean of this distribution?
mean(abs(median_50 - mean(median_50)) < 1.96*se_50)

## Make a plot
tibble(median_50) %>%
ggplot(aes(x=median_50)) +
  geom_density(fill=viridis3[1]) +
  ggtitle("Sample median, n=50",
          subtitle="Chicago O'Hare hourly maximum waiting time") +
  theme_bw()
```

## Problems

### 1

> For the Chicago wait time data in the example:

### 1A

> Use simulation to find the standard error of the sampling distribution of the sample median if the sample size is $n=5$ and plot the sampling distribution.

```{r problem-1}
## Read and format the data
chicago = read_csv2("../../data/chicago-2018.csv") %>%
  filter(hour >= "0800 - 0900" & hour <= "1900 - 2000") %>%
  select(us_max_wait) %>%
  rename(wait = us_max_wait)

## Calculate the population median
chicago %>%
  summarize(median = median(wait))

## Use simulation to estimate the sampling distribution of the sample median.

## Make a vector with the data we have from which we want to sample
chi_wait = chicago %>%
  pull(wait)

## Create the number of times we want to do the simulation
##   and the sample size as objects in R
B = 10000
n = 5

## Create a container to hold the sample median wait times
median_50 = numeric(B)

## Use a for loop to do the simulation
for ( i in 1:B )
{
  median_50[i] = median( sample(chi_wait, n, replace=TRUE) )
}

## The standard deviation of this sample is a numerical estimate of the standard error of the sampling distribution
se_50 = sd(median_50)
se_50

## Make a plot
tibble(median_50) %>%
ggplot(aes(x=median_50)) +
  geom_density(fill=viridis3[1]) +
  ggtitle("Sample median, n=5",
          subtitle="Chicago O'Hare hourly maximum waiting time") +
  theme_bw()
```
### 1B

> Describe the shape of the sampling distribution:
where is the mode or modes, describe any skew.

**Response:** It's skewed to the right. The mode seems to be around 35 (where the graph is spiked). 

### 1C

> Are about 95% of the observations within 1.96 standard errors of the distribution?

```{r}
mean(abs(median_50 - mean(median_50)) < 1.96*se_50)
```

**Response:** Yes. 

### 2

> Repeat problem 1, but use the 0.90 quantile from a sample of size 50.

```{r problem-2}
## Read and format the data
chicago = read_csv2("../../data/chicago-2018.csv") %>%
  filter(hour >= "0800 - 0900" & hour <= "1900 - 2000") %>%
  select(us_max_wait) %>%
  rename(wait = us_max_wait)

## Calculate the population median
chicago %>%
  summarize(median = median(wait))

## Use simulation to estimate the sampling distribution of the sample median.

## Make a vector with the data we have from which we want to sample
chi_wait = chicago %>%
  pull(wait)

## Create the number of times we want to do the simulation
##   and the sample size as objects in R
B = 10000
n = 50

## Create a container to hold the sample median wait times
median_50 = numeric(B)

## Use a for loop to do the simulation
for ( i in 1:B )
{
  median_50[i] = quantile( sample(chi_wait, n, replace=TRUE), 0.9 )
}

## The standard deviation of this sample is a numerical estimate of the standard error of the sampling distribution
se_50 = sd(median_50)
se_50

## Are about 90% of the sample medians within 1.96 SEs of the mean of this distribution?
#mean(abs(median_50 - mean(median_50)) < 1.65*se_50)

## Make a plot
tibble(median_50) %>%
ggplot(aes(x=median_50)) +
  geom_density(fill=viridis3[1]) +
  ggtitle("Sample median, n=5",
          subtitle="Chicago O'Hare hourly maximum waiting time") +
  theme_bw()
```

### 2B

> Describe the shape of the sampling distribution:
where is the mode or modes, describe any skew.

**Response:** The shape of the sampling distribution is a bimodal distribution, but skews slightly to the right. The graph dips and then spikes at 83. The modes are 78 and 83.

### 1C

> Are about 95% of the observations within 1.96 standard errors of the distribution?

```{r}
mean(abs(median_50 - mean(median_50)) < 1.96*se_50)
```

**Response:** Yes. 

### 3

> Theory informs us that the mean and variance of a chi-square distribution with $v$ degrees of freedom are $v$ and $2v$, respectively. Use the function `rchisq()` to take samples of size 100,000 random chi-square random variables when $v$ is 1, 5, and 10 and see if the results are consistent with theory.
Use `mean()` and `var()` to calculate the mean and variance of the samples.

```{r problem-3}
df1 = rchisq(100000, 1)

df5 = rchisq(100000, 5)

df10 = rchisq(100000, 10)

dat = tibble(mean_df1 = mean(df1),
             mean_df5 = mean(df5),
             mean_df10 = mean(df10),
             v_df1 = var(df1),
             v_df5 = var(df5),
             v_df10 = var(df10))

dat
```

**Response:** The results are consistent with the theory. 

### 4

> For large samples and under some other assumptions, the likelihood ratio test statistic has an approximate chi-square distribution. But this might not hold for small samples. Consider the following data set of five draws from a binomial distribution with $n=3$ and $p$ unknown.
$\mathbf{x} = (1,0,3,3,0)$, where the possible values of $x$ are 0, 1, 2, and 3 and the following hypotheses:

$$
H_0: X_i \sim \text{Binomial}(3,p) \quad \text{for $i=1,\ldots,5$}
$$
$$
H_a: X_i \sim F \quad \text{for $i=1,\ldots,5$}
$$

> where $F$ is a distribution with parameters $p_x$ for $x=0,1,2,3$ and $P(X=x) = p_x$.

> The following code will calculate the likelihood ratio test statistic.

```{r lrt}
## Estimate p from a tabulated binomial sample
est_p0 = function(x)
{
  n = length(x) - 1
  p_hat = sum((0:n)*x)/(n*sum(x))
  return ( p_hat )
}

## Estimate all p_i from a tabulated sample
est_p1 = function(x)
{
  p = x/sum(x)
  return ( p )
}

## Calculate G, the LRT test statistic
lrt = function(x)
{
  n = length(x) - 1
  p0 = est_p0(x)
  logl_0 = sum(x[x>0]*dbinom(0:n,n,p0,log=TRUE)[x>0])
  p1 = est_p1(x)
  logl_1 = sum(x[x>0]*log(p1)[x>0])
  G = 2 * (logl_1 - logl_0)
  return( G )
}
```

> Here is a calculation for the sample data.

```{r example}
x = c(1,0,3,3,0)
tab_x = tabulate(x+1,4)
test_stat = lrt(tab_x)
test_stat
```

> Answer the questions embedded in the code.

```{r problem-4, fig.height=2}
B = 10000

### 4A. What does the next line of code do?
lrt_sample = numeric(B)
###

n = 3
p0 = est_p0(x)
m = length(x)

for ( i in 1:B )
{
  ### 4B. Why is the sample generated from a binomial distribution?
  x_sample = rbinom(m,n,p0)
  ###
  lrt_sample[i] = lrt(tabulate(x_sample+1,n+1))
}

df = tibble(lrt = lrt_sample)

### 4C. Describe differences between the approximate density of the simulated sampling distribution (black curve) with the chi-square density with two degrees of freedom
ggplot(df, aes(x=lrt)) +
  geom_density() +
  ### 4D. Why does the chi-square distribution for comparison have 2 degrees of freedom?
  geom_chisq_density(2) +
  geom_chisq_fill(2, alpha = 0.2, fill = "blue") +
  geom_vline(xintercept = test_stat,
             color="red", linetype="dashed") +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  theme_minimal()

### 4E. What would the mean and variance of the simulated LRT sampling distribution values be close to if this simulated distribution was well approximated by the theoretical chi-square distribution?
mean(lrt_sample)
var(lrt_sample)

### P-value by simulation
mean(lrt_sample >= test_stat)
### P-value by chi-square distribution
1 - pchisq(test_stat,2)

### 4F. Why does the chi-square p-value calculation use 1 - pchisq()?
```
#### 4A

Creates an empty vector that is "B" long.

#### 4B

It's easier to resample from a binomial distribution. 

#### 4C

Because the black curve has several modes it has a more irregular density than the chisquare density curve. 

#### 4D

It best fits the shape of the black curve. 

#### 4E

The mean should be 2 and the variance should be 4. 

#### 4F

By subtracting the pchisq() from 1 we get the probability of the p-value being greater than the acceptance value. 
